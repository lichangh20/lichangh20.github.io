---
permalink: /
title: "Cheng Qian"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is Cheng Qian's personal homepage. <!--Please also refer to my [homepage@Tsinghua](https://www....).--> 

## A short introduction
I am a undergraduate student studying at Tsinghua University, majoring in Computer Science and Technology. I am a member of THUNLP, advised by Prof. Zhiyuan Liu. I have broad interests in natural language processing, with a focus on pre-trained language models, parameter & data efficient tuning, and their applications.

My latest research focused on the tool creation ability of large language models ([paper](https://arxiv.org/pdf/2305.14318.pdf)). Previously I have worked on how outdated wight coulg be recycled for during the lifelong pre-training of language models (ACL 2023 [paper](https://arxiv.org/pdf/2305.08702.pdf)), and explored the mode connectivity of modelsâ€™ various minima (EMNLP 2022 [paper](https://arxiv.org/pdf/2210.14102.pdf)). I have also participated in the survey paper on tool learning with foundation models ([paper](https://arxiv.org/pdf/2304.08354.pdf)).

I am currently looking for a PhD position in NLP, starting from Fall 2024. Please feel free to contact me if you are interested in my research.

<b>Research Highlights:</b>
* Comprehensive analysis of mode connectivity in pre-trained language models.
* Extensive use of data-efficient tuning methods to improve the model performance.
* Form the task of compatible tuning and propose methods to recycle outdated weights.
* Explore the foundational model's ability of using external tools under various scenarios.
* Propose the CREATOR framework to disentangle the model's abstract and concrete reasoning abilities.

## Publications
(*indicates equal contribution)

**Cheng Qian**, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji. CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation. ([Paper](https://arxiv.org/pdf/2305.14318.pdf))

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, **Cheng Qian**, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun. Tool Learning with Foundation Models. Submitted to Nature Machine Intelligence, under review. ([Paper](https://arxiv.org/pdf/2304.08354.pdf) / [Code](https://github.com/OpenBMB/BMTools))

Yujia Qin\*, **Cheng Qian**\*, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Recyclable Tuning for Continual Pre-training. ACL 2023 findings. ([Paper](https://arxiv.org/pdf/2305.08702.pdf) / [Code](https://github.com/thunlp/RecyclableTuning))

Yujia Qin\*, **Cheng Qian**\*, Jing Yi\*, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Exploring Mode Connectivity for Pre-trained Language Models. EMNLP 2022. ([Paper](https://arxiv.org/pdf/2210.14102.pdf) / [Code](https://github.com/thunlp/Mode-Connectivity-PLM))

## For more information
More info about Cheng Qian can be found in [CV](https://qiancheng0.github.io/cv/) or [downloaded CV](http://qiancheng0.github.io/files/CV_ChengQian.pdf).
