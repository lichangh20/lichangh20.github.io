---
permalink: /
title: "Cheng Qian"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is Cheng Qian's personal homepage. <!--Please also refer to my [homepage@Tsinghua](https://www....).--> 

## A short introduction
I am an undergraduate student studying at Tsinghua University, majoring in Computer Science and Technology. I am a member of [THUNLP](http://nlp.csai.tsinghua.edu.cn/), advised by [Prof. Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/). My research interests primarily lie in the field of natural language processing, with a particular focus on pre-trained language models, parameter and data-efficient tuning, tool learning and tool creation of large language models.

In my latest research, I have been investigating the tool creation ability of large language models, as outlined in our [paper](https://arxiv.org/pdf/2305.14318.pdf). Previously, I have also explored the concept of recycling outdated weights during the lifelong pre-training of language models, which was published in ACL 2023 findings ([paper](https://arxiv.org/pdf/2305.08702.pdf)). Additionally, I have contributed to research on the mode connectivity of models' various minima, which was presented at EMNLP 2022 ([paper](https://arxiv.org/pdf/2210.14102.pdf)). Moreover, I participated in a survey paper on tool learning with foundation models, which can be found [here](https://arxiv.org/pdf/2304.08354.pdf).

Looking forward, I am seeking a Ph.D. position in NLP, starting from Fall 2024. If you are interested in my research or would like to explore potential collaborations, please feel free to reach out to me.

<b>Research Highlights:</b>
* Comprehensive analysis of mode connectivity in pre-trained language models.
* Extensive use of data-efficient tuning methods to improve the model performance.
* Form the task of compatible tuning and propose methods to recycle outdated weights.
* Explore the foundational model's ability of using external tools under various scenarios.
* Propose the CREATOR framework to disentangle the model's abstract and concrete reasoning abilities.

## Publications
(*indicates equal contribution)

**Cheng Qian**, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji. CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation. ([Paper](https://arxiv.org/pdf/2305.14318.pdf))

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, **Cheng Qian**, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun. Tool Learning with Foundation Models. Submitted to Nature Machine Intelligence, under review. ([Paper](https://arxiv.org/pdf/2304.08354.pdf) / [Code](https://github.com/OpenBMB/BMTools))

Yujia Qin\*, **Cheng Qian**\*, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Recyclable Tuning for Continual Pre-training. ACL 2023 findings. ([Paper](https://arxiv.org/pdf/2305.08702.pdf) / [Code](https://github.com/thunlp/RecyclableTuning))

Yujia Qin\*, **Cheng Qian**\*, Jing Yi\*, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Exploring Mode Connectivity for Pre-trained Language Models. EMNLP 2022. ([Paper](https://arxiv.org/pdf/2210.14102.pdf) / [Code](https://github.com/thunlp/Mode-Connectivity-PLM))

## For more information
More info about Cheng Qian can be found in [CV](https://qiancheng0.github.io/cv/) or [downloaded CV](http://qiancheng0.github.io/files/CV_ChengQian.pdf).
