---
permalink: /
title: "Cheng Qian"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is Cheng Qian's personal homepage. <!--Please also refer to my [homepage@Tsinghua](https://www....).--> 

## A short introduction
I am a undergraduate student studying at Tsinghua University, majoring in Computer Science and Technology. I am a member of THUNLP, advised by Prof. Zhiyuan Liu. I have broad interests in natural language processing, with a focus on pre-trained language models, & data efficient tuning, and their applications.

My latest research focused on the mode connectivity of models’ various minima, and the related co-first author [paper](https://arxiv.org/pdf/2210.14102.pdf) was accepted by EMNLP 2022 main conference. Recently I am also conducting research on instruction tuning and the compatibility of language models, aiming to further investigate pre-trained models’ interpretability and linguistic potential.

<b>Research Highlights:</b>
* Comprehensive analysis of mode connectivity in pre-trained language models.
* Extensive use of data-efficient tuning methods to improve the model performance.
* Form the task of compatible tuning and propose methods to recycle outdated weights.

## Publications
(*indicates equal contribution)

Yujia Qin\*, **Cheng Qian**\*, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Recyclable Tuning for Continual Pre-training. *submitted to ACL 2023, under review*. 

Yujia Qin\*, **Cheng Qian**\*, Jing Yi\*, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Exploring Mode Connectivity for Pre-trained Language Models. *EMNLP 2022*. ([Paper](https://arxiv.org/pdf/2210.14102.pdf) / [Code](https://github.com/thunlp/Mode-Connectivity-PLM))

## For more information
More info about Cheng Qian can be found in [CV](https://qiancheng0.github.io/cv/) or [downloaded CV](http://qiancheng0.github.io/files/CV_ChengQian.pdf).
