---
layout: archive
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Download [CV](http://qiancheng0.github.io/files/CV_ChengQian.pdf)
======

Education
======
* B.Eng. in Computer Science and Technology, Tsinghua University, Beijing, China, 2020-2024 (expected)

Academic
======
* GPA: 3.89 / 4.00.
* Selected Courses of A & A+: Linear Algebra, Introduction to Complex Analysis, Foundation of Object-Oriented Programming, Software Engineering, Introduction to Artificial Intelligence, Artificial Neural Networks, Fundamentals of Computer Graphics, A General Introduction to Economics, Writing and Communication.
* A member of THUNLP (THU Natural Language Processing Group), advised by Associate Professor Zhiyuan Liu.

Research Interests
======
* Natural language processing, pre-trained language models.
* Data efficient tuning and training efficient tuning (tuning with limited resources).
* Life long pre-training and continual learning in NLP.
* interpretability of the language model, and its alignment to human instructions.

Publications
======
(*indicates equal contribution)

* Yujia Qin*, Cheng Qian*, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Recyclable Tuning for Continual Pre-training. *submitted to ACL 2023, under review*. 

* Yujia Qin*, Cheng Qian*, Jing Yi*, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun and Jie Zhou. Exploring Mode Connectivity for Pre-trained Language Models. EMNLP 2022. ([Paper](https://arxiv.org/pdf/2210.14102.pdf) / [Code](https://github.com/thunlp/Mode-Connectivity-PLM))



Research Experience
======
* Mar 2022 – Jul 2022: Exploring Mode Connectivity for Pre-trained Language Models
  * Directed by Associate Professor Zhiyuan Liu, THUNLP.
  * Analyzed the geometric connections of different minima in loss landscape through the lens of mode connectivity, which measures whether two minima can be connected with a low loss path.
  * Explored how various hyperparameters and training data affect PLMs’ mode connectivity; Discovered the role of pre-training in facilitating mode connectivity and pulling task boundaries closer; Investigated into how PLMs task knowledge change along the connected path quantitatively.
  * Co-first author. Accepted by EMNLP 2022 main conference.
  * Project established in THU Student Research Training Program.

* Aug 2022 – Jan 2023: Recyclable Tuning for Continual Pretraining		 		           
  * Directed by Associate Professor Zhiyuan Liu, THUNLP.
  * Formulated the task of compatible tuning as PLM continually acquire fresh knowledge from emerging data, and explored how to make earlier adapted weights compatible with subsequent upgraded PLMs.
  * Explored the parametric connections among continually pre-trained models; Proposed CLoP, which enables compatible tuning in a data-efficient and training efficient way; Experimented on various NLP tasks and demonstrated the superiority of CLoP; Construct the first benchmark regarding to the field of compatible tuning.
  * Co-first author. Applying for an invention patent. Related paper in submission.
  * Project selected to THU Undergraduate Academic Advancement program and won ￥30K support.

* Mar 2022 – Jun 2022: THUPat: A Convenient Campus Helper						        
  * Directed by Associate Researcher Chun Yu, Theory and Practice of Human Computer Interaction course project.
  * Proposed “pat” for the first time as the medium in human-phone interaction. Built an open source android software THUPat that can help with various kinds of campus events via simply patting the phone.
  * Collaborator. Software released in THU and benefited the campus community.

* Jul 2022 – Aug 2022: Quantum Automata: Capability and Efficiency 					         
  * Directed by Professor Zhengfeng Ji and Professor Mingsheng Ying, Topics in Quantum Computing course project.
  * Defined the efficiency of quantum automata from 3 different perspectives with respect to acceptance probability, space and time; Proposed an algorithm that can effectively optimize quantum automata’s acceptance probability, applying the knowledge from neural network.
  * First author. Course thesis won high recognition.

<!--
Publications
======
  <ul>{% for post in site.publications %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
 
Talks
======
  <ul>{% for post in site.talks %}
    {% include archive-single-talk-cv.html %}
  {% endfor %}</ul>
  
Teaching
======
  <ul>{% for post in site.teaching %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
-->

Awards
======
* December-9th Scholarship, highest scholarship in Dept. of CST, 2 out of 180. (2021)
* Volunteering & Social Survey Excellence Scholarship, Dept. of CST, 2 out of 180. (2022)
* Awards of Excellent Student Cadre, Tsinghua University. (2021)
* Second Prize in National Undergraduate Physics Competition, Beijing Physics Society. (2021)
* Third Prize in THU Challenge Cup Academic Competition, Tsinghua University. (2022)

Languages
======
Mandarin(Native), English(Fluent)
* TOEFL  113/120 (Reading 30, Listening 30, Speaking 24, Writing 29).
* GRE  Verbal Reasoning 162/170, Quantitative Reasoning 170/170, Analytical Writing 4/6.

Skills & Expertise 
======
* Highly self-motivated researcher. 
* Strong interpersonal skills with a good sense of teamwork.
* Programming Skills: Python, C/C++.
* Rich experience in state-of-the-art deep learning techniques.

Service and leadership
======
* Serve as COLING 2022 reviewer.

